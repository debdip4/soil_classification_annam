{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5781bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soil or Not-Soil Detection using Soil-Type Classifier\n",
    "# import libraries\n",
    "import torch\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations for paths and model settings\n",
    "TEST_DIR = \"C:\\\\Users\\\\debdi\\\\Downloads\\\\soil_competition-2025\\\\test\"\n",
    "TEST_IDS_CSV = \"C:\\\\Users\\\\debdi\\\\Downloads\\\\soil_competition-2025\\\\test_ids.csv\"\n",
    "MODEL_PATH = \"C:\\\\Users\\\\debdi\\\\Downloads\\\\soil_competition-2025\\\\best_model.pth\"\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CLASSES = ['Alluvial soil', 'Black Soil', 'Clay soil', 'Red soil']\n",
    "\n",
    "# Image preprocessing for input to the model\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),    # Resize all images to 224x224\n",
    "    transforms.ToTensor(),            # Convert images to PyTorch tensors\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])      # Normalize to pretrained stats\n",
    "])\n",
    "\n",
    "# Custom dataset class for loading test images\n",
    "class TestSoilDataset(Dataset):\n",
    "    def __init__(self, img_dir, test_ids_csv, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.test_ids_df = pd.read_csv(test_ids_csv)\n",
    "        self.image_ids = self.test_ids_df['image_id'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_name\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = TestSoilDataset(TEST_DIR, TEST_IDS_CSV, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b09a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\debdi\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\debdi\\AppData\\Roaming\\Python\\Python313\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Predicting: 100%|██████████| 31/31 [00:37<00:00,  1.21s/it]\n"
     ]
    }
   ],
   "source": [
    "# The model was trained to classify 4 types of soil (Red, Black, Alluvial, Clay) for Challenge 1\n",
    "# For Challenge 2, we repurpose this model to detect if an image is a soil image or not\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Predict Probabilities and Convert to Binary Labels\n",
    "predictions = []\n",
    "threshold = 0.90  # High confidence threshold for soil\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, image_ids in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        images = images.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        max_probs, _ = torch.max(probs, dim=1)\n",
    "        labels = (max_probs > threshold).long().cpu().numpy()  # 1 = soil, 0 = not soil\n",
    "\n",
    "        for img_id, label in zip(image_ids, labels):\n",
    "            predictions.append((img_id, label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f94dda51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete. Submission saved to C:\\Users\\debdi\\Downloads\\soil_competition-2025\\submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Save Submission\n",
    "submission_df = pd.DataFrame(predictions, columns=[\"image_id\", \"label\"])\n",
    "submission_path = \"C:\\\\Users\\\\debdi\\\\Downloads\\\\soil_competition-2025\\\\submission.csv\"\n",
    "submission_df.to_csv(submission_path, index=False)\n",
    "print(f\"Inference complete. Submission saved to {submission_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
